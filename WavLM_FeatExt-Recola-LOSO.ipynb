{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc547b1-84d4-4ba6-a8a1-8877423fe056",
   "metadata": {},
   "source": [
    "\n",
    "# WavLM Feature Extractor for RECOLA\n",
    "\n",
    "### For LOSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760109a1-c818-49cf-abd3-377b65994a33",
   "metadata": {},
   "source": [
    "#### WavLM model\n",
    "##### https://github.com/microsoft/unilm/tree/master/wavlm\n",
    "\n",
    "#### LOSO example\n",
    "##### https://github.com/audeering/w2v2-how-to/blob/main/notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2a1c8e-b2fd-416a-8b2d-73a3728923a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run WavLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8263d9b9-de2b-418e-b1a5-da5323499a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37b7839d-4542-4c68-ba35-35d34d64483a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "NVIDIA RTX A6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/livia/home/ecrit/akoerich/anaconda3/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WavLM(\n",
       "  (feature_extractor): ConvFeatureExtractionModel(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)\n",
       "        (3): GELU(approximate='none')\n",
       "      )\n",
       "      (1-4): 4 x Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "      (5-6): 2 x Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
       "        (1): Dropout(p=0.0, inplace=False)\n",
       "        (2): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (dropout_input): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_features): Dropout(p=0.1, inplace=False)\n",
       "  (encoder): TransformerEncoder(\n",
       "    (pos_conv): Sequential(\n",
       "      (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      (1): SamePad()\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (relative_attention_bias): Embedding(320, 12)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-11): 11 x TransformerSentenceEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (dropout_module): Dropout(p=0.1, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (grep_linear): Linear(in_features=64, out_features=8, bias=True)\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from WavLM import WavLM, WavLMConfig\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load('model/WavLM-Base+.pt')\n",
    "cfg = WavLMConfig(checkpoint['cfg'])\n",
    "model = WavLM(cfg)\n",
    "#model = model.to(device) #, dtype=torch.float32)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aab6830b-334b-4148-8f3f-89e19c10a476",
   "metadata": {},
   "source": [
    "import torch\n",
    "#from WavLM import WavLM, WavLMConfig\n",
    "\n",
    "# load the pre-trained checkpoints\n",
    "checkpoint = torch.load('WavLM/model/WavLM-Base+.pt')\n",
    "cfg = WavLMConfig(checkpoint['cfg'])\n",
    "model = WavLM(cfg)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "\n",
    "# extract the representation of last layer\n",
    "wav_input_16khz = torch.randn(1,2600000)\n",
    "if cfg.normalize:\n",
    "    wav_input_16khz = torch.nn.functional.layer_norm(wav_input_16khz , wav_input_16khz.shape)\n",
    "rep = model.extract_features(wav_input_16khz)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02707503-babb-4372-9aca-486b656482dd",
   "metadata": {},
   "source": [
    "rep[0].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4806bcc9-2e65-41cf-b322-c0cb3cd32153",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rep_np = rep[0].detach().numpy()\n",
    "rep_df = pd.DataFrame(rep_np)\n",
    "rep_df.to_csv('tmp.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "079534a7-5acb-4e3b-bd6e-753965b19b91",
   "metadata": {},
   "source": [
    "# extract the representation of each layer\n",
    "wav_input_16khz = torch.randn(1,16000)\n",
    "if cfg.normalize:\n",
    "    wav_input_16khz = torch.nn.functional.layer_norm(wav_input_16khz , wav_input_16khz.shape)\n",
    "rep, layer_results = model.extract_features(wav_input_16khz, output_layer=model.cfg.encoder_layers, ret_layer_results=True)[0]\n",
    "layer_reps = [x.transpose(0, 1) for x, _ in layer_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99264fb6-920e-4302-bdec-a8f391af8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to source files\n",
    "path2 = 'Recola2018_16k/audio/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5009cede-ddf6-4e26-be23-bbba55a00573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed: 9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Read a directory and put all files in a list\n",
    "file_list = []\n",
    "i = 0 \n",
    "for path, subdirs, files in os.walk( path2 ):\n",
    "    for name in files:\n",
    "        file_list.append( os.path.join( path, name) )\n",
    "        i += 1\n",
    "\n",
    "# Sort by filename\n",
    "sorted_file_list = sorted(file_list)\n",
    "file_list = sorted_file_list\n",
    "        \n",
    "print(\"Files processed: \"+str(i) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb09937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recola2018_16k/audio/Test/test_1.wav',\n",
       " 'Recola2018_16k/audio/Test/test_2.wav',\n",
       " 'Recola2018_16k/audio/Test/test_3.wav',\n",
       " 'Recola2018_16k/audio/Test/test_4.wav',\n",
       " 'Recola2018_16k/audio/Test/test_5.wav',\n",
       " 'Recola2018_16k/audio/Test/test_6.wav',\n",
       " 'Recola2018_16k/audio/Test/test_7.wav',\n",
       " 'Recola2018_16k/audio/Test/test_8.wav',\n",
       " 'Recola2018_16k/audio/Test/test_9.wav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0feee48-fbad-405e-8791-6811e58105dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files processed: 9\n",
      "Average file length: 4800000.0 samples   300.0 s   5.0 min\n",
      "Max length: 4800000 samples   300.0 s   5.0 min\n",
      "Min length: 4800000 samples   300.0 s   5.0 min\n"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "# sample rate = 16,000\n",
    "#  1s = 16,000 x 1 =  16,000\n",
    "# 60s = 16,000 x 60 = 960,500\n",
    "\n",
    "length = list()\n",
    "i      = 0\n",
    "avg    = 0\n",
    "\n",
    "for file in file_list:\n",
    "    data, samplerate = sf.read( file )\n",
    "    \n",
    "    if len(data) <= 960500:                         \n",
    "            print(\"Audio length: \"+str(len(data))+\" with less than 30s: \"+str(file) )\n",
    "    #computer average lenght of files\n",
    "    avg = avg + len(data)\n",
    "    length.append(len(data))\n",
    "    i += 1\n",
    "\n",
    "print( \"Files processed: \"+str(i) )\n",
    "print( \"Average file length: \"+str(avg/i) + \" samples   \"+str(avg/i/samplerate)+\" s   \"+str(avg/i/samplerate/60)+\" min\" )\n",
    "print( \"Max length: \"+str(max(length))+ \" samples   \"+str(max(length)/samplerate)+\" s   \"+str(max(length)/samplerate/60)+\" min\" )\n",
    "print( \"Min length: \"+str(min(length))+ \" samples   \"+str(min(length)/samplerate)+\" s   \"+str(min(length)/samplerate/60)+\" min\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c62bde6a-7e5d-4052-b21c-38723e7746f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_1.wav\n",
      "Recola2018_16k/audio/Test/test_1.wav\n",
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_2.wav\n",
      "Recola2018_16k/audio/Test/test_2.wav\n",
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_3.wav\n",
      "Recola2018_16k/audio/Test/test_3.wav\n",
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_4.wav\n",
      "Recola2018_16k/audio/Test/test_4.wav\n",
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_5.wav\n",
      "Recola2018_16k/audio/Test/test_5.wav\n",
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_6.wav\n",
      "Recola2018_16k/audio/Test/test_6.wav\n",
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_7.wav\n",
      "Recola2018_16k/audio/Test/test_7.wav\n",
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_8.wav\n",
      "Recola2018_16k/audio/Test/test_8.wav\n",
      "--------------\n",
      "Sample Rate: 16000 Length: (4800000,) Recola2018_16k/audio/Test/test_9.wav\n",
      "Recola2018_16k/audio/Test/test_9.wav\n"
     ]
    }
   ],
   "source": [
    "sampling_rate = 16000\n",
    "track_count = 0\n",
    "\n",
    "for file in file_list:\n",
    "    data , samplerate = sf.read( file )\n",
    "    print (\"--------------\")\n",
    "    print (\"Sample Rate: \" + str(samplerate) + \" Length: \" + str(data.shape) + \" \" + str( file ) )\n",
    "\n",
    "    # extract the representation of last layer\n",
    "    wav_input = torch.from_numpy(data).float()\n",
    "    wav_input_16khz = torch.unsqueeze(wav_input,0)\n",
    "    # wav_input_16khz = torch.randn(1,48000)\n",
    "    # wav_input_16khz = torch.randn(1,2786987)\n",
    "    if cfg.normalize:\n",
    "        wav_input_16khz = torch.nn.functional.layer_norm(wav_input_16khz , wav_input_16khz.shape)\n",
    "\n",
    "    #rep = model.extract_features(wav_input_16khz.to(device, dtype=torch.float32))[0]\n",
    "    rep = model.extract_features(wav_input_16khz)[0]\n",
    "\n",
    "    rep[0].shape\n",
    "    rep_np = rep[0].detach().numpy()\n",
    "    rep_df = pd.DataFrame(rep_np)\n",
    "    subject = file.split('/')[3].split('.')[0]\n",
    "\n",
    "    # Add subject column\n",
    "    rep_df.insert(0, \"Subject\", subject, True)\n",
    "    \n",
    "    file_id = 'Recola2018_16k/features/'+subject+'.wavlmbasefeatloso'  \n",
    "    rep_df.to_csv(file_id)\n",
    "    \n",
    "    # Pooling \n",
    "    df_temp = pd.DataFrame(rep[0].detach().numpy())\n",
    "    df_pool = df_temp.rolling(2, step=2).mean().drop(index=0) \n",
    "    \n",
    "    # Add subject column\n",
    "    df_pool.insert(0, \"Subject\", subject, True)\n",
    "    \n",
    "    file_id = 'Recola2018_16k/features/'+file.split('/')[3].split('.')[0]+'.wavlmbasefeatpoolloso'  \n",
    "    df_pool.to_csv(file_id)\n",
    "    \n",
    "    print( file )\n",
    "    \n",
    "    track_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eba7794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4794d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model.pth\t\tWavLM_FeatExt.ipynb\r\n",
      "best_model_recola.pth\tWavLM_FeatExt-Recola.ipynb\r\n",
      "best_model_sewa.pth\tWavLM_FeatExt-Recola-LOSO.ipynb\r\n",
      "features\t\tWavLM.py\r\n",
      "model\t\t\tWavLM_Train_Model-BiLSTM-Recola.ipynb\r\n",
      "modules.py\t\tWavLM_Train_Model-GRU.ipynb\r\n",
      "newfile.txt\t\tWavLM_Train_Model-GRU-Recola.ipynb\r\n",
      "__pycache__\t\tWavLM_Train_Model-GRU-Recola-LOSO.ipynb\r\n",
      "Recola2018_16k\t\tWavLM_Train_Model.ipynb\r\n",
      "SEWA16\t\t\tWavLM_Train_Model-LSTM.ipynb\r\n",
      "test_dataset.predict\tWavLM_Train_Model-LSTM-old.ipynb\r\n",
      "test_dataset.txt.model\tWavLM_Train_Model-ThunderSVM.ipynb\r\n",
      "ThunderSVM.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb08c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abd91d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5100af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb803e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a3d96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d3a438c",
   "metadata": {},
   "source": [
    "### Process Feature Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f108093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to feature files\n",
    "path_train = 'features/train'\n",
    "extension = 'wavlmbasefeatpool'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67436984",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [file for file in os.listdir(path_train) if file.endswith(extension)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train_files = sorted(train_files)\n",
    "sorted_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577acc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for file in sorted_train_files:\n",
    "    df = pd.read_csv(os.path.join(path_train, file))\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074617de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60744de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat.drop(df_feat.columns[[0]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c362c",
   "metadata": {},
   "source": [
    "### Process Label files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3528871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to label files\n",
    "path_train_labels = 'SEWA16/labels/Train/'\n",
    "extension = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f5b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files_labels = [file for file in os.listdir(path_train_labels) if file.endswith(extension)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fbf416",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_train_labels = sorted(train_files_labels)\n",
    "sorted_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94feb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = []\n",
    "for file in sorted_train_labels:\n",
    "    df2 = pd.read_csv(os.path.join(path_train_labels, file), sep=\";\")\n",
    "    df2.drop(df2.columns[[0,1]], axis=1, inplace=True)\n",
    "    dfl.append(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaaaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe82ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab = pd.concat(dfl, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30534e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204f607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab.drop(df_lab.columns[[0,1,4]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e31af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ca41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c4ba2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842464e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf883d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create classifier and grouping object\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    SVC(gamma='auto'),\n",
    ")\n",
    "logo = LeaveOneGroupOut()\n",
    "\n",
    "def experiment(\n",
    "    features,\n",
    "    targets,\n",
    "    groups,\n",
    "):        \n",
    "    truths = []\n",
    "    preds = []\n",
    "    \n",
    "    # leave-one-speaker loop    \n",
    "    pbar = audeer.progress_bar(\n",
    "        total=len(groups.unique()),\n",
    "        desc='Run experiment',\n",
    "    )\n",
    "    for train_index, test_index in logo.split(\n",
    "        features, \n",
    "        targets, \n",
    "        groups=groups,\n",
    "    ):\n",
    "        train_x = features.iloc[train_index]\n",
    "        train_y = targets[train_index]\n",
    "        clf.fit(train_x, train_y)\n",
    "        \n",
    "        truth_x = features.iloc[test_index]\n",
    "        truth_y = targets[test_index]\n",
    "        predict_y = clf.predict(truth_x)\n",
    "        \n",
    "        truths.append(truth_y)\n",
    "        preds.append(predict_y)\n",
    "        \n",
    "        pbar.update()\n",
    "        \n",
    "    # combine speaker folds\n",
    "    truth = pd.concat(truths)\n",
    "    truth.name = 'truth'\n",
    "    pred = pd.Series(\n",
    "        np.concatenate(preds),\n",
    "        index=truth.index,\n",
    "        name='prediction',\n",
    "    )\n",
    "    \n",
    "    return truth, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356040c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth_w2v2, pred_w2v2 = experiment(\n",
    "    df,\n",
    "    emotion,\n",
    "    speaker,\n",
    ")\n",
    "audformat.utils.concat([truth_w2v2, pred_w2v2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4115fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c85d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df5dab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a4e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
